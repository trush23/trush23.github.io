[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 8, 2026\n\n\nTrush\n\n\n\n\n\n\n\n\n\n\n\n\nEnsembles: Part II.\n\n\n\nphysics\n\nai\n\n\n\n\n\n\n\n\n\nSep 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEnsembles: Part I\n\n\n\nphysics\n\nai\n\n\n\n\n\n\n\n\n\nAug 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPhysics in Machine Learning, Deep Learning, and AI.\n\n\n\nphysics\n\nai\n\n\n\n\n\n\n\n\n\nAug 8, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "outreach.html",
    "href": "outreach.html",
    "title": "Outreach",
    "section": "",
    "text": "I regularly give talks at high school, create research projects with high school students, and lead math events for the local community. I also give public lectures at the New York Public Library, collaborate as a mentor with New York Academy of Sciences, Pioneer Academy, and Center for Mathematical Talent. Several high school students I have done research projects with, have been semi finalists in the INTEL Science Competition.\n\nArchaeoHack (2025): Mentor and judge for the inaugural ArchaeoHack 2025. The topic was to create an app that recognizes Egyptian Hieroglyphs as images or sketched by hand.\nCSPLASH (2021 onwards): Math Society volunteers organized annual day-long event for highschool students; talks by faculty, lab visits, and games.\nGirls in STEM (GSTEM): A Courant Institute Summer research program for female high school students in the US. Participated as research mentor in 2013, 2014, 2015, 2016.\nNew York Public Library (NYPL): Gave two lecture series (5 lectures each) at the Jefferson Branch. (Spring 2016) Mathematical Patterns In Nature, (Fall 2016) Physics of Everyday Things.\nNew York Academy of Sciences (NYAS) 2017-2021:\n\nGlobal Science Academy: As a research mentor, I mentored research team composed of high school students around the world on\nresearch projects (online, 2017, 2018).\nScientist-In-Residence: Mentored 7th and 8th grade classes in New York Public Schools on research projects for one semester. PS-181 in Fall 2017 and PS-151 in Fall 2018.\nAfter-School Program (2019): Participated in after school program for 2nd, 3rd, and 4th grade students, guiding them through activities and experiments.\n\nNYU-ABU DHABI: Mentoring and teaching talented Emerati high school students in Abu Dhabi (Summer 2017).\nCenter for Mathematical Talent: Lecture Series on Graph Theory for talented high school students in NYC (2016, 2018, 2020, 2022).\n\n\n\n\n\n\n\n\n\n\n\n\nPioneer Academy\n\n\n\nOutreach\n\nHigh School Research\n\nData Science\n\n\n\nTwo months of summer school for six students, teaching Deep Learning and AI, and six individual research projects.\n\n\n\nJun 1, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses/Linear-Algebra.html",
    "href": "courses/Linear-Algebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Syllabus (PDF)\nLecture Notes"
  },
  {
    "objectID": "courses/Linear-Algebra.html#course-materials",
    "href": "courses/Linear-Algebra.html#course-materials",
    "title": "Linear Algebra",
    "section": "",
    "text": "Syllabus (PDF)\nLecture Notes"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Education Projects",
    "section": "",
    "text": "Here are the major educational initiatives I am currently leading or participating in.\n\n\n\n\n\n\n\n\n\n\n\n\nCourse-Based Undergraduate Research Framework (CURE)\n\n\n\nPedagogy\n\nCurriculum Development\n\nUndergraduate Research\n\n\n\n\n\n\n\nJan 20, 2026\n\n\n\n\n\n\n\n\n\n\n\nCustom Gen AI Tutor for Linear Algebra\n\n\n\nPedagogy\n\nCurriculum Development\n\nAI\n\n\n\n\n\n\n\nSep 1, 2025\n\n\n\n\n\n\n\n\n\n\n\nRestructuring Linear Algebra\n\n\n\nPedagogy\n\nCurriculum Development\n\n\n\n\n\n\n\nSep 1, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/nypl-patterns-2017.html",
    "href": "talks/nypl-patterns-2017.html",
    "title": "Mathematical Patterns in Nature",
    "section": "",
    "text": "In this series of lectures, we will explore how similar mathematical patterns emerge; from galaxies to atoms, in art and in finance. We will first explore how certain numbers like “pi” and “e” keep appearing in diverse fields. [cite_start]We will see how numbers and their sequences give an underlying mathematical structure to different problems.\nNext we will examine the role of randomness in mathematics and our lives. We will see how from this randomness emerges a regular pattern - the famous “Bell Curve”. We will investigate its uses in different situations. We will then explore how mathematics allows us to deconstruct complex situations involving decision-making. How do we come up with a method for fair distribution, be it for property, or corporate merger, or organ transplant.\n[cite_start]In the fourth lecture, we will study an unusual system of arithmetics called “modular arithmetic”, which allows us to use prime numbers to create the ubiquitous security and code systems that are in every bar code, every credit card - the so called “RSA encryption”.\nFinally, we will study the world of graphs and networks: similar patterns and properties emerge in networks of different types, from airline routes to electrical grids. We will also see the use of graph theory to solve minimal cost, minimal time, shortest path, and task-scheduling problems."
  },
  {
    "objectID": "talks/nypl-patterns-2017.html#abstract",
    "href": "talks/nypl-patterns-2017.html#abstract",
    "title": "Mathematical Patterns in Nature",
    "section": "",
    "text": "In this series of lectures, we will explore how similar mathematical patterns emerge; from galaxies to atoms, in art and in finance. We will first explore how certain numbers like “pi” and “e” keep appearing in diverse fields. [cite_start]We will see how numbers and their sequences give an underlying mathematical structure to different problems.\nNext we will examine the role of randomness in mathematics and our lives. We will see how from this randomness emerges a regular pattern - the famous “Bell Curve”. We will investigate its uses in different situations. We will then explore how mathematics allows us to deconstruct complex situations involving decision-making. How do we come up with a method for fair distribution, be it for property, or corporate merger, or organ transplant.\n[cite_start]In the fourth lecture, we will study an unusual system of arithmetics called “modular arithmetic”, which allows us to use prime numbers to create the ubiquitous security and code systems that are in every bar code, every credit card - the so called “RSA encryption”.\nFinally, we will study the world of graphs and networks: similar patterns and properties emerge in networks of different types, from airline routes to electrical grids. We will also see the use of graph theory to solve minimal cost, minimal time, shortest path, and task-scheduling problems."
  },
  {
    "objectID": "talks/nypl-patterns-2017.html#event-details",
    "href": "talks/nypl-patterns-2017.html#event-details",
    "title": "Mathematical Patterns in Nature",
    "section": "Event Details",
    "text": "Event Details\nLocation: Jefferson Branch, Manhattan Audience: All ages."
  },
  {
    "objectID": "projects/curriculum-reform.html",
    "href": "projects/curriculum-reform.html",
    "title": "Restructuring Linear Algebra",
    "section": "",
    "text": "Traditional way of teaching Linear Algebra, at most colleges, follows covering certain foundational material from standard textbooks like those by Strang or Lay. While this approach has been the mainstay of undergraduate curriculum, it no longer serves for the benefit of the large number of majors who are opting to learn Linear Algebra for multiple reasons. For one, the traditional approach leans too much towards theoretical aspects and secondly, in this day and age, it makes more sense to give students a glimpse of its foundational role in data science, machine learning, and applications in general.\nIn the summer 2024, together with a working group of fellow faculty members, I proposed my plan of restructuring Linear Algebra by making it more applied and including Python coding component for the course. We implemented the new version starting Fall 2025. In the last three semesters, it has been a resounding success! The students love the restructured course and the enrollments have significantly increased with each semster.\n\n\n\nOpen-source online textbook: Interactive Linear Algebra\nUse a library of open-source Python notebooks: Jupyter Guide to Linear Algebra\nApplications based Final Project with Jupyter Notebooks: Pagerank, Curve Fitting, SVD image compression, Dynamical Systems, Differential Equations, Markov Processes, etc."
  },
  {
    "objectID": "projects/curriculum-reform.html#project-overview",
    "href": "projects/curriculum-reform.html#project-overview",
    "title": "Restructuring Linear Algebra",
    "section": "",
    "text": "Traditional way of teaching Linear Algebra, at most colleges, follows covering certain foundational material from standard textbooks like those by Strang or Lay. While this approach has been the mainstay of undergraduate curriculum, it no longer serves for the benefit of the large number of majors who are opting to learn Linear Algebra for multiple reasons. For one, the traditional approach leans too much towards theoretical aspects and secondly, in this day and age, it makes more sense to give students a glimpse of its foundational role in data science, machine learning, and applications in general.\nIn the summer 2024, together with a working group of fellow faculty members, I proposed my plan of restructuring Linear Algebra by making it more applied and including Python coding component for the course. We implemented the new version starting Fall 2025. In the last three semesters, it has been a resounding success! The students love the restructured course and the enrollments have significantly increased with each semster.\n\n\n\nOpen-source online textbook: Interactive Linear Algebra\nUse a library of open-source Python notebooks: Jupyter Guide to Linear Algebra\nApplications based Final Project with Jupyter Notebooks: Pagerank, Curve Fitting, SVD image compression, Dynamical Systems, Differential Equations, Markov Processes, etc."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "The following are Public Lecture Series I gave at the New york Public Library, Jeffereson Branch.\n\n\n\n\n\n\n\n\n\n\n\nThe Physics of Everyday Things\n\n\n\nPublic Lecture\n\nNew York Public Library\n\n\n\nA series of five lectures on the physics of everyday materials all around us like sand, soap, shampoo, ketchup, toothpaste. What makes them so different from each other in…\n\n\n\nFeb 2, 2018\n\n\n\n\n\n\n\n\n\n\nMathematical Patterns in Nature\n\n\n\nPublic Lecture\n\nNew York Public Library\n\n\n\nA lecture course (5 lectures) given at the New York Public Library, Jefferson Branch, about mathematical patterns that naturally emerge in nature, from arrangement of petals…\n\n\n\nSep 2, 2017\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is my blog using Quarto. I have a blog on Substack but I will be using my blog here to write them and upload them on Substack. It is way easier to write math with Quarto."
  },
  {
    "objectID": "posts/phyai-post-2/index.html",
    "href": "posts/phyai-post-2/index.html",
    "title": "Ensembles: Part II.",
    "section": "",
    "text": "(Note: This post is a bit long)\nIn my previous post (Ensembles Part I), I described at the basic level, the way ensembles are used in physics. In this post, I will explore how the same concept is implemented in ML, DL, and AI. There has been a lot of work done on the statistical mechanics of Deep Learning and AI, but those are naturally very mathematical and quite advanced [1,2,3]. Here, I am restricting myself to examples of ensemble approach in different models and methods.\nBefore I begin, a historical sidebar is in order:\nAs a student, I got introduced to ensembles via Statistical Mechanics in physics and naturally assumed that one of the luminaries of statistical mechanics must have invented it. Recently though, while reading some very old papers (a longtime hobby of mine and for background research for another publication I plan to start), I came across a public lecture by James Clerk Maxwell [4], who credits statisticians processing census data with inventing this method of ensembles in early to mid 1800s. I look at this not as a game of who did it first but a natural consequence of intersection of ideas. The whole of AI is built upon the foundations of Statistics, Mathematics, Computer Science, and Physics. The relevant extract of the lecture is at the end of this post.\nBack to the main theme…\nThere are two fundamental aspects to consider when we apply ML methods to any problem.\nThe first is to recognize and remember that in ML and AI, we are always trying to find and analyze patterned states.\nThis is the biggest point of divergence between systems in Physics and ML. In physics we are typically dealing with systems at equilibrium or states of maximum entropy, whereas in any ML, DL, AI, the goal is to identify, classify, predict patterns. A patterned state by definition is one of lower entropy compared to randomly distributed data.\nIt is not that physics models can’t deal with patterns, of course they do, from all sorts of waves to crystal structures and turbulent flow etc. etc. but when we are dealing with systems not at equilibrium, at the very least, we have to be careful how we extend the methods of equilibrium situations. Classic examples of this are non-equilibrium thermodynamics and pattern formation in dynamical systems.\nThe second aspect is to differentiate between the machinery/algorithm/model used to catch those patterns and the pattern itself. A snapshot of a cat is a patterned state compared to random pixel values, but a model to classify it as a cat has its own training dynamics and ideally it reaches its own equilibrium state when it has “recognized” that pattern. Given that we stop the training at some specified point using some cutoffs, it is probably more accurate to say that the model reaches its own “steady state”.\nA dynamical or non-equilibrium thermodynamic steady state is different than true equilibrium. It is a state that while being patterned, maintains that pattern for a long time (indefinitely in principle unless disturbed). Think of zebra stripes or the vortex of a tornado, or even our body temperature or the body itself. Incidentally, the grandfather of AI, Alan Turing, was the first one to model and analyze patterns generated out of chemical reactions using Reaction-Diffusion equations [5].\nNow we can look at some specific situations of ensembles in ML and AI.\nThere are four main types of data that get used in pretty much all of AI— text, images, tabular data, and audio. Video can be considered essentially as a combination of the above. Irrespective of the data modality, in majority of the cases, we are not looking for any kind of exact function that describes something (unlike in math or physics). We are usually performing statistical inferencing or “solving” the issue at hand in a statistical manner. We are answering questions like how many times the model correctly describes the data or what is the most likely event or outcome given what we know has happened. Whether it is about predicting the next word or identifying cats vs dogs, or forecasting stock prices, we “train” our models to some satisfactory accuracy and use them to answer questions in a statistical manner with some confidence level.\nOne way or another, pretty much every model or method, requires either creation of or partitioning of data in ways that amount to creating ensembles. Apart from a few models like Random Forests, this subtle issue is not explicitly mentioned but it is present nonetheless. I will elaborate this point by using the following examples:\n\nRandom Forests\nData Augmentation\nSurrogate and Synthetic Data\nGenerative AI\n\nI will not go into text data or time series data because both require their own separate treatment; because of the way we use language in a context dependent manner for text data and the importance of chronological order in time series data. I will just point out that creation of ensembles is an essential first step because it is about data preparation and data processing and the success of a model critically depends on it.\nRandom Forests (RF):\nSuppose you are trying to analyze an issue that may (or may not) depend on a bunch of variables. You are not sure how many variables or which ones are the most relevant. So you collect data for all of the possible variables or do what you can. For the sake of concreteness, lets say we have some data about people and we have both categorical and numerical variables like age, gender, highest degree, annual income, height, weight, marital status, employment, favorite color, favorite cuisine, etc. etc. The question we need answered is not terribly important here, but in principle, we could make a single decision tree by factoring in every single variable and running through that decision tree. At worst, we may have to assign some arbitrary cutoffs for numerical variables to decide whether to go down one path or the other. It is possible but somewhat fixed and rigid process.\nThis is where Random Forests come in [6]. Instead of dealing with all the variables at once, we make a decision tree with a few variables, a subset of total number of variables. We do this at each node where we are deciding which path to follow. The collection of all such decisions trees is called, for obvious reasons, a Random Forest. But in essence, we have created an ensemble of decision trees.\nWe do this because a single decision tree is susceptible to “over fitting”, meaning it works mostly for that dataset and that question. It is also susceptible to giving wildly different answers when we make tiny changes to the data—– it has high variance. When we create an ensemble of decision trees, we are in effect saying that we don’t know which variables are the most relevant and which ones are not, let that emerge from the process. Since we also run through many randomized combinations of variables at each node, the variance decreases. Occasionally, we may get a wildly different result but generally (and hopefully) they will follow some sensible distribution with a well-defined peak or most probable outcome.\nThis may not look like the “gas in a box” scenario at first glance, but we are going through “micro-states” of trees and an outlier result is like all of the gas in a corner! What is different is that instead of one fixed macro variable, we have a distribution of final answers. What we do with that depends on whether the problem was regression or classification. For regression problem, we want our “micro-states” to inform us about a certain relationship between dependent and independent variables and we get to that desired outcome by imposing minimum standard error (MSE) requirement. For classification problems, instead of one fixed macro variable value, we have two classes (for simplicity), and every outcome is put in one class or the other and misclassification error is minimized.\nA sidebar:\nA cute demo of something similar to RFs is what’s known as Galton’s Board. A particle is dropped through wooden pegs. Depending on which path it takes, it lands somewhere, but when we repeat the experiment for a large number of particles, or create ensembles and average them out, we get a Gaussian distribution. The most probable outcome is peaked right below the root node. Check out: https://imgur.com/gallery/galton-board-uALvLTe and https://en.wikipedia.org/wiki/Galton_board.\nThere are too many subtle nuances in going from DTs to RFs in terms of implementation, efficiency of algorithm, splitting the data into training and testing, cost function to minimize, etc. Each of these in turn relies on the fact that we are working with ensembles and a priori it is not obvious how should we go about creating them. All such attempts at improving the basic method give rise to new approaches like bagging, gradient boosting, and the most popular of them all, XGBoost. I will go over the details of statistical methods within RFs in a dedicated post about it later in this series.\nData Augmentation:\nAlmost every DL and AI model dealing with images employs “data augmentation”. While it doesn’t say so explicitly, when we do that, we are creating or expanding an ensemble. Let’s see why it can be seen as such with images as a typical example.\nLet’s say we are dealing with recognizing cats vs. dogs. We have a bunch of pictures of each category. Irrespective of what model we use, the first thing we do is to standardize and normalize those images in terms of size, intensity values, etc. Now, the logic goes that while we have these specific pictures, it could have been a slightly shifted picture, or maybe the camera was at an angle and the picture got rotated, or it was an extreme closeup etc. but it is still the same cat! Computationally, we apply various transformations like horizontal and vertical shifts, scaling, rotations, shearing, flipping, etc.\nRecall from earlier that any ensemble is a collection of “snapshots” of the system and by doing data augmentation we are filling potentially missing but valid variations (micro-states) of the system. The caveat here is that this correspondence with statistical mechanical idea of ensembles of micro-states is not exact. In almost every problem, there are always some variations that are downright impossible or impractical and misleading at the very least. This is why we don’t apply any random transformation we feel like but stick with a select set of transformations that seem to make the ensemble and the method better. For example, if we are working on identifying cars on the road from images, flipping the images and showing upsidedown cars is unrealistic and probably not very useful. Another quick example is the figure below. You will never see different patches of an image randomly scrambled up as an “augmented” image, even though the distribution of pixel intensities is exactly the same.\n\n\n\nCat vs. Augmented Cat\n\n\nThere is a mathematical basis for this in terms of types of transformations that preserve certain features and the types that don’t. Notice that in some sense, we still need to keep some things constant as we create our ensembles, like the temperature or pressure in physical systems. Here, exactly what needs to stay constant (more or less) is not clear. Sometimes it is the probability distribution of some feature or features, the entire data, certain symmetries in the data, some mathematical structure or some as yet unknown thing that makes the whole thing work. We don’t know in general. More on all that in another dedicated post later.\nResampling, Surrogate Data, and Synthetic Data:\nIn tabular data, we can’t generally do data augmentation arbitrarily. We can’t randomly add fake employees to create bigger HR dataset! What we can do on occasion is to create multiple copies of the data with changes in the order of the data (if the order is not critical) or equivalent changes to some features. Again, it is not that straightforward to know whether we are somehow introducing more bias in the system or not. The idea is to maintain the distribution of the features and the dataset as a whole. adding similar data is called the surrogate method. It is used not just in tabular data but in time series data as well. There is a long history of analyzing chaotic dynamical systems using surrogate data. A collection of such copies of original data will again be an ensemble. The more representative the ensemble is the better.\nSynthetic data is based purely on the idea that the certain features follow some specific distribution of values. The idea is to create datasets by pulling numbers from these distributions randomly and create an ensemble of datasets.\nFinally, the most basic step in any method is the “train-test” split or a separate validation set also included sometimes. There is usual practice is to do something like 80/20 split, where 80% of the data is used for training and 20% for testing. Since the training data is picked at random (while keeping various caveats in mind), we can get an ensemble of train-test pair. Each time a model is run on a slightly different training dataset (Resampling), we will get a different performance on the test dataset. This is for a different reason than the model converging on some small value of error or a high value of accuracy at later epochs.\nWe still don’t have a first-principles theory of the whole connection between data ensembles and ML/AI methods. Even within one specific model, we don’t have the full theory like in statistical physics, but we do know that elements from equilibrium and non-equilibrium statistical physics do seem to be applicable to strangely different problems like identifying cats vs. dogs.\nWe need to only look at generative AI to know this. Creating a new image from a supplied image or text is built upon principles of non-equilibrium thermodynamics and diffusion (hence the name “stable diffusion”). The idea is to add noise to the starting system, let it evolve and guide it to a different steady state via diffusion. Obviously, this is an oversimplified explanation and there is rigorous math and statistical physics involved in it but here I just want to mention that generative AI as we have it would not work without it.\nOver the course of this series of posts, I will revisit each of these concepts individually with key mathematics and physics concepts that are involved and that seem to make these models work even though we don’t know exactly how or why. We do know that if we apply these concepts as if our problems were similar to a thermodynamic system and if we tune everything just right (hyper-parameter tuning), then we can do all sorts of things like detecting diseases to forecasting sales figures; anything that requires finding patterns buried within the data.\nIn my next post, I will explore the concept of entropy. A much used concept found in both physics and information science and used extensively in ML/AI in multiple ways.\nIn the meanwhile, a few questions to ponder over:\n\nThink of image augmentation. At what point, as we keep transforming say a cat picture, it is no longer recognizable as a cat? Is it an abrupt process or gradual? What is changing mathematically (shape, size, colors, orientation?) and context-wise as we keep transforming an image? Is there anything that still stays constant? What should an ensemble for ML/AI problems be like (preferred properties)?\nCan we have more dramatic transformations like the picture below where we block out certain areas? This is what is known as occlusion studies, where only a part of the object is visible. Here, we are obviously changing the distribution of pixel intensities as well, yet we can easily identify a cat. The question is how to make an algorithm identify it.\n\n\n\n\nCat covered with blobs\n\n\n\nWe haven’t talked about text data but common sense tells us that just to create multiple variations of a sentence, and hence an ensemble, we cannot randomly shuffle around letters because then the meaning is destroyed. Can we create an ensemble by adding a bunch of useless words to a sentence and call it “another version”? How about using synonyms for every word whenever possible?\n\nThe main point behind these questions is to emphasize that we actually don’t know for sure. We have hints and clues and sometimes mathematical or contextual reasons for all of the questions above but no theory from some kind of first principles, which is why pretty much everything, even very simple questions, are still or should be topics of research.\nAs promised at the beginning, pictures of James Clerk Maxwell’s Lecture on Diffusion, where he talks about Section F (statistics division) coming up with the ensemble approach. Hope you enjoyed this rather long post.\n\n\n\nCat vs. Augmented Cat\n\n\nReferences:\n\n“Statistical Mechanics of Deep Learning”, Bahri et al., Annual Review of Condensed Matter\nPhysics (2020).11:501–28.\n“Statistical mechanics of deep learning”, Freya Behrens et al., J. Stat. Mech. (2024) 104007.\n“SETOL: A Semi-Empirical Theory of (Deep) Learning”, Charles Martin and Christopher Hinrichs, arXiv:2507.17912v2 [cs.LG], (2025).\n“MOLECULES”, James Clerk-Maxwell, The College Courant, Nov. 8, 1873, Vol. 13, No. 17, pp. 193-197.\n“Random Decision Forests”, Tin Kam Ho, Proceedings of 3rd International Conference on Document Analysis and Recognition, Montreal, QC, Canada, 1995, pp. 278-282 vol.1.\n“The chemical basis of morphogenesis”, Phil. Trans. R. Soc. Lond. B 237:37-72."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching Activities",
    "section": "",
    "text": "Below is a list of courses I have taught over the years:\n\nMath Courses Taught and Coordinated:\n\nPre-Calculus\nCalculus I, II, and III\nMath for Economics I, II, and III\nDiscrete Mathematics\nLinear Algebra\n\nQuantitative Reasoning Courses:\n\nFrom Data to Discovery (FTDT)\nProbability, Statistics, and Decision Making (PSDM)\nGreat Ideas in Mathematics (GIM)\n\nMasters Level Course: Mathematical Statistics"
  },
  {
    "objectID": "teaching.html#courses",
    "href": "teaching.html#courses",
    "title": "Teaching Activities",
    "section": "",
    "text": "Below is a list of courses I have taught over the years:\n\nMath Courses Taught and Coordinated:\n\nPre-Calculus\nCalculus I, II, and III\nMath for Economics I, II, and III\nDiscrete Mathematics\nLinear Algebra\n\nQuantitative Reasoning Courses:\n\nFrom Data to Discovery (FTDT)\nProbability, Statistics, and Decision Making (PSDM)\nGreat Ideas in Mathematics (GIM)\n\nMasters Level Course: Mathematical Statistics"
  },
  {
    "objectID": "teaching.html#courses-in-spring-2026",
    "href": "teaching.html#courses-in-spring-2026",
    "title": "Teaching Activities",
    "section": "Courses in Spring 2026",
    "text": "Courses in Spring 2026\n\nLinear Algebra\nGreat Ideas in Mathematics\nFrom Data To Discovery"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "I have split the “Research” portion into three components. My past research had a different, professional goal. I started off as an experiemntal physicist but I have worked with applied mathematicians and engineers.\nCurrently, I am more focused on using research as a teaching tool for undergrads and high school students. I create appropriate research projects for motivated and sincere students to give them research experience, improve critical thinking skills, and teach them how to explore and investigate a research problem.\nRecently, I have been exploring problems at the intersection of Math, Physics, and Deep Learning. These topics are timely and particularly suitable for undergrads and high schoolers for short projects.\n\nProfessionalWith UndergradsWith High School Students\n\n\n\n\n\nStress Chains in 2d Granular Systems\n\n\nMy primary area falls broadly under the label of “soft matter physics”. I have worked in three different areas: Granular physics (Ph.D.), Rheology and jetting dynamics of complex fluids, and fluid-structure interactions in the swimming motion of C. elegans.\nMy Ph.D. was on granular physics. I solved the highly complex and seemingly intractable inverse problem of finding contact forces on particles from photoelastic images. My Ph.D. work was considered groundbreaking, that resulted in my very first publication in Nature. A follow up work on jamming in granular media was published in Physical Review Letters as a Cover Article.\n Download Thesis\nPublications:\n\nT. S. Majmudar and R. P. Behringer, “Contact Force Measurements and Stress Induced Anisotropy in Granular Materials”, Nature, 435 1079-1082, (2005)  Download Paper\nMajmudar, T. S. and Sperl M. and Luding S. and Behringer R. P. “Jamming transition in granular systems”, Physical Review Letters, 98, 058001 (2007) (Cover Article).  Download Paper\nG. Lois, J. Zhang, T. S. Majmudar, S. Henkes, B. Chakraborty, C. S. O’Hern, R. P. Behringer, “Stress correlations in granular materials: an entropic formulation”, Physical Review E , 80, 060303(R), (2009).  Download Paper\nJ. Zhang, T. S. Majmudar, M. Sperl, and R. P. Behringer, “Jamming for a 2D granular material”, Soft Matter, 6, 2982-2991, (2010).  Download Paper\n\nRheology of Complex Fluids and Jetting Dynamics.\n\n\n\nCoiling of a fluid jet\n\n\n\nC. J. Pipe, T. S. Majmudar, and G. H. McKinley “High Shear Rate Viscometry”, Rheologica Acta, 47, 5, 621-642, (2008).  Download Paper\nT. S. Majmudar and G. H. McKinley, “Nonlinear Dynamics of Coiling in Viscoelastic Jets”, arxiv  Download Paper\nMatthew Varagnat, T. S. Majmudar, Will Hartt, G. H. McKinley “The folding motion of axisymmetric jets of micellar solutions”, arxiv  Download Paper\n\nLocomotion of C. elegans in structured media.\n\n\n\nLocomotion of worms in a structured medium\n\n\n\nTrushant Majmudar and Eric Keaveny, Mike Shelley, Jun Zhang, “ Experiments and Theory of Undulatory Locomotion of Microorganisms in Structured Media”, Royal Soc. J. of Interface (2012).  Download Paper\n\n\n\n\nXumo Pan (2025) Tokenizing Intelligence: Valuing AI Prompts and Models as On-chain Real-World Assets.\nHaihong Pang (2024) Study of Image Similarity Measures\nBoyoon Han (2024) Study of Cosine Similiarity in NLP\nSisi Qiu (2024) Data Analysis of International Student Enrollment\nAria Han (2023) Spiking Neuronal Networks on complete and random graphs.\nZhilin Chen (2023) Perception under Occlusion: Comparison between brain imaging studies and deep learning methods.\nRuinan Lu (2015) Graph theory analysis of chromatin genetic networks.\nOlivia Chu (2014) Imaging and analysis of locomotion by shape deformation in Euglena.\nHarshita Kajaria (2014) Nonlinear dynamics in financial time series.\nMonty Liu (2013) Stress analysis of assembly of elliptical particles subjected to concentrated loads.\nSunjeong Bonna Kim (2012) Collective swimming of C. elegans.\n\n\n\nIntel Science Competition: - Rishad Rahman, Bronx Science High School. Rishad recently graduated from MIT with a major in Computer Science. Project: Collective locomotion of Euglena Gracialis. We collected data on the swarming behavior of Euglena and analyzed the videos to examine the properties of their collective swimming.\n\nTherese Chan, Bronx Science High School. Therese is currently an undergrad at Carnegie Mellon, Com- puter Science. Project: Automatic detection of force distribution in granular systems - a Matlab implementation. We took an existing C code I had written while doing my PH.D. at Duke and converted parts of it in to a Matlab code to automate the process of finding contact forces in granular systems.\n\nGSTEM Program:\n\nSophie Connor, Schoharie High School. Sophie is an undergraduate and a joint Math/CS major at Dartmouth. Project: Making Music from Chaos. We numerically iterated a few one dimensional chaotic maps like the Henon map and the logistic map and converted the outputs of those maps in diﬀerent behavioral windows like the periodic, quasi-periodic, chaotic etc. into sounds by using a midi converter.\nPenny Mian, Hicksville Public School. Penny is now at Georgetown University in Chemical Engineering. Project: Pattern formation in Belusov-Zhabotinsky reaction. We carried out experiments and analyzed pattern formation in this reaction.\nAileen Venegas. Aileen is now at George Washington University in Biomedical Engineering. Project: Network analysis of gene architecture in the chromosome. We analyzed connection network of gene organization in chromosomes from real experimental data (provided by a collaborator).\n\nPioneer Academy:\n\nTairan Ma: “Multimodal Machine Learning for Physiological Privacy Protection in Facial Videos”\nYichen Leng: “Data Analytics for Speed-Cubing Rankings”\nMohammed Ali: “Machine Learning for Memristor Networks”\nXiaodan (Amy) He: “Convolutional Neural Networks for Facial Expression Recognition: Analysis of Current CNNs and Integration of AI Enhancement”\nTerry Duan: “Retrieval-Augmented Generation for Intelligent Learning Tools”"
  },
  {
    "objectID": "research.html#past-and-current-research",
    "href": "research.html#past-and-current-research",
    "title": "Research",
    "section": "",
    "text": "I have split the “Research” portion into three components. My past research had a different, professional goal. I started off as an experiemntal physicist but I have worked with applied mathematicians and engineers.\nCurrently, I am more focused on using research as a teaching tool for undergrads and high school students. I create appropriate research projects for motivated and sincere students to give them research experience, improve critical thinking skills, and teach them how to explore and investigate a research problem.\nRecently, I have been exploring problems at the intersection of Math, Physics, and Deep Learning. These topics are timely and particularly suitable for undergrads and high schoolers for short projects.\n\nProfessionalWith UndergradsWith High School Students\n\n\n\n\n\nStress Chains in 2d Granular Systems\n\n\nMy primary area falls broadly under the label of “soft matter physics”. I have worked in three different areas: Granular physics (Ph.D.), Rheology and jetting dynamics of complex fluids, and fluid-structure interactions in the swimming motion of C. elegans.\nMy Ph.D. was on granular physics. I solved the highly complex and seemingly intractable inverse problem of finding contact forces on particles from photoelastic images. My Ph.D. work was considered groundbreaking, that resulted in my very first publication in Nature. A follow up work on jamming in granular media was published in Physical Review Letters as a Cover Article.\n Download Thesis\nPublications:\n\nT. S. Majmudar and R. P. Behringer, “Contact Force Measurements and Stress Induced Anisotropy in Granular Materials”, Nature, 435 1079-1082, (2005)  Download Paper\nMajmudar, T. S. and Sperl M. and Luding S. and Behringer R. P. “Jamming transition in granular systems”, Physical Review Letters, 98, 058001 (2007) (Cover Article).  Download Paper\nG. Lois, J. Zhang, T. S. Majmudar, S. Henkes, B. Chakraborty, C. S. O’Hern, R. P. Behringer, “Stress correlations in granular materials: an entropic formulation”, Physical Review E , 80, 060303(R), (2009).  Download Paper\nJ. Zhang, T. S. Majmudar, M. Sperl, and R. P. Behringer, “Jamming for a 2D granular material”, Soft Matter, 6, 2982-2991, (2010).  Download Paper\n\nRheology of Complex Fluids and Jetting Dynamics.\n\n\n\nCoiling of a fluid jet\n\n\n\nC. J. Pipe, T. S. Majmudar, and G. H. McKinley “High Shear Rate Viscometry”, Rheologica Acta, 47, 5, 621-642, (2008).  Download Paper\nT. S. Majmudar and G. H. McKinley, “Nonlinear Dynamics of Coiling in Viscoelastic Jets”, arxiv  Download Paper\nMatthew Varagnat, T. S. Majmudar, Will Hartt, G. H. McKinley “The folding motion of axisymmetric jets of micellar solutions”, arxiv  Download Paper\n\nLocomotion of C. elegans in structured media.\n\n\n\nLocomotion of worms in a structured medium\n\n\n\nTrushant Majmudar and Eric Keaveny, Mike Shelley, Jun Zhang, “ Experiments and Theory of Undulatory Locomotion of Microorganisms in Structured Media”, Royal Soc. J. of Interface (2012).  Download Paper\n\n\n\n\nXumo Pan (2025) Tokenizing Intelligence: Valuing AI Prompts and Models as On-chain Real-World Assets.\nHaihong Pang (2024) Study of Image Similarity Measures\nBoyoon Han (2024) Study of Cosine Similiarity in NLP\nSisi Qiu (2024) Data Analysis of International Student Enrollment\nAria Han (2023) Spiking Neuronal Networks on complete and random graphs.\nZhilin Chen (2023) Perception under Occlusion: Comparison between brain imaging studies and deep learning methods.\nRuinan Lu (2015) Graph theory analysis of chromatin genetic networks.\nOlivia Chu (2014) Imaging and analysis of locomotion by shape deformation in Euglena.\nHarshita Kajaria (2014) Nonlinear dynamics in financial time series.\nMonty Liu (2013) Stress analysis of assembly of elliptical particles subjected to concentrated loads.\nSunjeong Bonna Kim (2012) Collective swimming of C. elegans.\n\n\n\nIntel Science Competition: - Rishad Rahman, Bronx Science High School. Rishad recently graduated from MIT with a major in Computer Science. Project: Collective locomotion of Euglena Gracialis. We collected data on the swarming behavior of Euglena and analyzed the videos to examine the properties of their collective swimming.\n\nTherese Chan, Bronx Science High School. Therese is currently an undergrad at Carnegie Mellon, Com- puter Science. Project: Automatic detection of force distribution in granular systems - a Matlab implementation. We took an existing C code I had written while doing my PH.D. at Duke and converted parts of it in to a Matlab code to automate the process of finding contact forces in granular systems.\n\nGSTEM Program:\n\nSophie Connor, Schoharie High School. Sophie is an undergraduate and a joint Math/CS major at Dartmouth. Project: Making Music from Chaos. We numerically iterated a few one dimensional chaotic maps like the Henon map and the logistic map and converted the outputs of those maps in diﬀerent behavioral windows like the periodic, quasi-periodic, chaotic etc. into sounds by using a midi converter.\nPenny Mian, Hicksville Public School. Penny is now at Georgetown University in Chemical Engineering. Project: Pattern formation in Belusov-Zhabotinsky reaction. We carried out experiments and analyzed pattern formation in this reaction.\nAileen Venegas. Aileen is now at George Washington University in Biomedical Engineering. Project: Network analysis of gene architecture in the chromosome. We analyzed connection network of gene organization in chromosomes from real experimental data (provided by a collaborator).\n\nPioneer Academy:\n\nTairan Ma: “Multimodal Machine Learning for Physiological Privacy Protection in Facial Videos”\nYichen Leng: “Data Analytics for Speed-Cubing Rankings”\nMohammed Ali: “Machine Learning for Memristor Networks”\nXiaodan (Amy) He: “Convolutional Neural Networks for Facial Expression Recognition: Analysis of Current CNNs and Integration of AI Enhancement”\nTerry Duan: “Retrieval-Augmented Generation for Intelligent Learning Tools”"
  },
  {
    "objectID": "posts/intro/index.html",
    "href": "posts/intro/index.html",
    "title": "Physics in Machine Learning, Deep Learning, and AI.",
    "section": "",
    "text": "Let’s see if the following scenario sounds familiar or relatable:\nYou are a current student who realized that you need to learn about Machine Learning (ML), Deep Learning (DL), and AI in general. Maybe you are a mid-career professional who is looking to pivot, realizing the same or maybe you have to learn something about these things because you got “the memo” that you will be working with newly hired AI expert!\nChances are that irrespective of the exact situation, we all end up following a similar plan of action: look at free resources on the internet, Youtube videos, Lecture notes of famous courses, maybe a bootcamp or two, maybe a paid course if you can afford to. After all that, you feel reasonably secure that you know when to apply which method, why different subject areas like computer vision or healthcare require specific methods suited to those areas. You may even attempt to read original research papers on the methods you intend to use. That’s when you start realizing that the entire structure is heavily steeped in Math!\nYou had a feeling about it but you pushed it back to be dealt with later. Now, you start to notice how many people were talking about getting a foundation in math to be able to use ML/DL/AI more effectively and interpret the results properly. So you dust off the old books on calculus, linear algebra, and probability and statistics and repeat the whole cycle mentioned above, but this time for math. How much math you need to do depends on your own individual case. Unless you are a researcher or an engineer tweaking these systems at the fundamental level, you only need an intuitive grasp of the concepts and how they become the foundation of ever complicated architectures.\nFinally, you feel you are set and have a reasonable foundation to build upon and go deeper. As soon as you do that, you encounter technical terms, jargon and concepts that were never explained in any material you studied or any math you learned! You come across terms like ensemble, temperature, entropy, partitions, free energy, attractors, stability, equilibrium, non-equilibrium, diffusion, manifolds, groups, symmetries, eigenspaces, dynamics, variational, renormalization, etc. You may even hear someone mention Ising Model, quantization, or pretty much anything with quantum as a prefix.\nNow you despair that also need to know Physics!\nIt does feel like this whole thing is a bottomless pit but there is a good reason for the need to know a bit of physics. All of the terms I mentioned above and many more are at the foundation of this whole field, starting with neural networks. Most of them are concepts borrowed from Physics and applied to DS/ML/DL/AI. It is a fact that starting with “neurons” in neural networks, a whole bunch of methods, architectures, and protocols are new applications of statistical physics. Occasionally, well-established methods from astronomy and particle physics have also been modified and adapted.\nIt is not so surprising that the Nobel Prize for advances in AI was awarded in the Physics category. Now, it may even be clearer why notable figures like Elon Musk and Jensen Huang have been advocating studying physics.\nI have been observing and thinking about this need to explain the physics that operates behind-the-scenes for a while now. In subsequent posts, I aim to comprehensively explain each and every physics concept that is used in AI. I am not attempting to convert anyone to study physics endlessly or become an expert at it. My goal for this blog is to give you intuitive understanding and sufficient insights so that if someone tries to “blind you with physics”, you can hold your own!\nLet me briefly explain why I think I can do this. Firstly, I love teaching, am passionate about it. Majority of my teaching has been in Math and Physics. I am trained as a soft matter physicist, but at various points in my life, I have worked in astrophysics, nonlinear dynamics and chaos, pattern formation, Bose-Einstein condensation, physics of granular systems, complex fluids, and locomotion of microorganisms. That basically covers the entire length scale, from cosmic scale to quantum scale and the in-between scale.\nI transitioned to a teaching career thirteen years ago. I am a teaching-track math professor at NYU, Courant Institute. I have taught math to an incredibly diverse group of undergraduates. I have taught abstract math concepts to every possible major on campus. I enjoy finding ways to explain and engage students who are usually apprehensive and sometimes terrified of learning math. I plan on bringing the same passion here and to quote Einstein, “Make everything as simple as possible but not simpler.”\nI genuinely hope you enjoy my offering and maybe even get some practical benefit out of it. I know I am going to thoroughly enjoy this new creative outlet."
  },
  {
    "objectID": "posts/phyai-post-1/index.html",
    "href": "posts/phyai-post-1/index.html",
    "title": "Ensembles: Part I",
    "section": "",
    "text": "Caveats:\nIn all of the subsequent posts in this publication, on occasion, I will need to use diagrams and figures. Most of the time, I will be relying on hand-drawn doodles instead of fancy diagrams and animations. For the most part, they will be enough for an intuitive understanding of the concepts. Making fancy diagrams and animations is a time-consuming and challenging project in itself and I don’t want to go down that rabbit hole and let it detract me from the main purpose of this publication.\nThese posts are written for learners who are unfamiliar or vaguely familiar with physics concepts used in ML/DL/AI. For that reason, my aim is primarily to impart an intuitive understanding and not technical exactness. On occasion, I will make statements that are generally true but with caveats and conditions. I will progressively refine those statements once a certain foundation has been established.\nTypically, any data science or machine learning course or program starts with supervised and unsupervised learning and topics like regression, clustering, and decision trees, to name a few. These methods, while being technical and mathematical, are not that difficult to understand intuitively. Logistic regression is a way to split your data into different classes. Clustering also groups the data into different clusters based on different criteria. Decision Trees (DT) are somewhat flow-charty way of doing what we do all the time— arrive at a decision based on answers to Yes/No type questions for various factors and conditions.\nDecision Trees and extension of DT, Random Forests (RF), is where we suddenly encounter the unfamiliar concept of an ensemble. We also have concepts like bagging, boosting, voting, stacking, etc. This concept of ensemble is everywhere in machine learning and AI. In a certain sense, we make use of it when we split the data into training-validation-testing, making batches of our data, introducing noise in a systematic way in images, image augmentation, and many more situations.\nSo, in this post and the follow-up post (Part II), I will talk about:\nWhat exactly is an ensemble? Why do we need ensembles? What does it have to do with physics? When is this concept used purely for statistical reasons and when does a bit of physics sneak in.\nDictionary definition of the word “ensemble” is just a collection or a group of items. In a way, that is how it is used in sciences as well as ML/AI, except with some caveats. In the examples mentioned above, I have briefly stated how ensembles are used in ML. I will describe how they are used in physics in this post. To do that, and explore other related concepts in forthcoming posts, I will make use of the classic “gas-in-a-box” example. It is a simple enough system to intuitively understand most of the statistical mechanics.\n\n\n\nGas in a Box\n\n\nConsider this box filled with molecules of a gas. Even without any fancy theories, we know that we always only see it spread throughout the box. If we saw that all of the gas collected itself in some corner, we would consider it as bizarre or spooky! More on that and its connection to entropy will be in subsequent posts. For now, let’s focus on how do we make this observation quantitative. Let us assume that all the atoms or molecules of the gas are identical— nothing distinguishes them from each other. So if we swap two of them, the “state” of the system doesn’t change.\nThe “state” of the system has to be clarified further. Statistical mechanics is the foundational theory of thermodynamics. In thermodynamics, we measure global or “macro” variables like temperature, pressure, volume, etc., usually at the boundaries of the system. A set of specific values of such variables would be the “macro-state” of the system. Now, once we make the box, the volume is fixed unless we start messing with the walls. Let’s say we keep the whole box in some temperature-controlled environment, so that the temperature is also fixed at some value. For simple modeling purposes, if we assume that the gas is made up of tiny particles bouncing around, then it turns out that temperature is connected to the kinetic energy of the particles, summed and averaged. Kinetic energy depends on the velocity of the particle (assuming that the mass m is constant).\n\\(E = \\dfrac{1}{2}mv^2 = \\dfrac{3}{2}kT\\)\nAt any time, a micro-state of the system is given by the position and velocity of every single particle. It is not difficult to imagine that if we want the temperature to be some fixed value T, that can be achieved by a gazillion different combinations of velocities. If one particle has high velocity, it can be compensated by another particle with low velocity to maintain the average. Each such combination of positions and velocities of particles of the gas is a “micro-state” of the system.\nThe important point is, many micro-states will produce the same value of the macro variable. In principle, the entire gas trapped in some corner leaving most of the box empty is a perfectly valid micro-state that produces the same temperature. Yet, we don’t see that happening spontaneously, ever! We will explore why that is later with the concept of entropy. For now, let’s return to how we could make quantitative measurements to verify this picture.\nThere are two main approaches:\n\n\n\nTracking a particle for a long time: Time Average\n\n\nThe first one is to pick a particle, any particle, since none of them are special, and follow (track) the particle for a long time; i.e measure its position at tiny time intervals, calculate its velocity from its location at two successive times, calculate the energy and find the average over time. This would amount to creating a time series for a particle. From this accompanying doodle it is not that outlandish to imagine that if we track this particle long enough, it will eventually visit all parts of the box somewhat “democratically”, with no particular preference for some locations. We can extend this to tracking more than just one particle and get even more accurate information. In essence, we are investigating the time evolution of a micro-state of the system and finding time-averages of micro-state variables like the velocity of the particle.\nThe second approach is the one that involves ensembles. The logic behind it, is that at any time, the system is frozen in some particular configuration. There isn’t anything particularly special about that configuration, so another slightly different configuration is also a good representative of the global state of the system. A single configuration is like a snapshot and a whole stack of such snapshots, each slightly different from each other, each conforming to the same macro variable values, creates what we call an ensemble. Instead of tracking a particle over time, we can study a large number of snapshots of the system, and calculate averages over the entire ensemble.\n\n\n\nEnsemble Average\n\n\nA fundamental result of statistical mechanics is that if the system is ergodic in nature, time average method give the same results as the ensemble average method when the system is in equilibrium with its surroundings. This is called the ergodic hypothesis and systems that follow this are called ergodic systems. When exactly does this happen? The exact treatment of this difficult mathematical concept is beyond this post, but roughly speaking, when a system can access all parts of the phase space (positions and velocities)— no regions are inaccessible. This is the main reason why ensemble method works, at least for simple well-defined systems at equilibrium, because what happens to a system over long time is also captured by looking at different realizations of the system (ensemble).\nThere are two main types of systems; one that evolves with time and another that is purely statistical or probabilistic in nature, where time is irrelevant. A gas in a box, until it reaches equilibrium, is a system that evolves in time. For a coin toss done hundreds of times, time is not relevant. The properties of systems like that are based on how many outcomes are possible, which outcome occurs at every trial, and how many times each outcome occurs. For systems that don’t involve time evolution, ensemble approach is the natural choice. In systems involving time, it may not always be possible to observe it for incredibly long time periods. The ensemble approach allows us to figure out the most likely behavior. Essentially, it is a powerful approach for both types of systems.\nFor systems in physics, the ensemble approach is used typically when there are some global variables kept constant and some allowed to change. Depending on that, we get different ensembles. For the same “gas-in-a-box” situation, we get canonical ensemble if the Temperature (T) is kept fixed and the system is allowed to exchange heat with a heat bath. At equilibrium, the system will have the same temperature as the heat bath. Number of particles (N) and Volume (V) are the other variables. There are other types of ensembles depending on whether total energy (E) is kept constant, volume is kept constant but N and E are allowed to change, pressure (P) is kept constant etc. The bottom line is in physical and chemical systems, ensemble method works best if the system is in equilibrium with its environment with some variables kept constant and some other variables allowed to vary, giving rise to a large number of micro-states that correspond to the macro-state at equilibrium.\nThe question then arises whether all of these micro-states are equally important or not. Depending on which type of ensemble we are dealing with, each micro-state is assigned a weight or a probability of occurring. For example, in canonical ensemble, the probability of each micro-state is given by P = exp((F - E)/kT). E is the total energy, F is the “free energy”, k is the Boltzmann constant and T is the temperature. The important point is the probabilities are an exponential function, not the same for every micro-state. This and some other calculations allow us to calculate what are the most probable micro-states; those are the states we will see pretty much always, even though bizarre states like the entire gas crowding in a corner are technically possible. The probability of such micro-states showing up is exceedingly low.\nThis was just the bare bones basics of the concept of ensembles as used in thermodynamics and statistical mechanics of systems in equilibrium. Needless to say, the formalism has been extended to account for many other complicated scenarios in many branches of physics and chemistry and even systems that are not at equilibrium, via non-equilibrium thermodynamics (some ideas from it go into generative AI).\nIn the next post, Part II on Ensembles, I will discuss how this concept is used in machine learning, how much of its usage relies on ideas from physics and how much is purely from probability theory and statistics."
  },
  {
    "objectID": "projects/Gen_AI_Tutor.html",
    "href": "projects/Gen_AI_Tutor.html",
    "title": "Custom Gen AI Tutor for Linear Algebra",
    "section": "",
    "text": "In Fall 2025, together with the office of Strategic Initiatives and Planning at NYU IT, we initiated a pilot program to incorporate Gen. AI Tutor trained on the textbook and other course-related documents, for one section of Linear Algebra. The tutor is designed to give only hints and suggestions and never the full answers right away. It guides the students towards solutions, which effectively improves their understanding of the material. Analysis of the data collected from that semester is underway and will be used to refine the Tutor for the upcoming Spring 2026 semester.\n\n\n\nCreate a Personalized Course Tutor.\nGuided and focused study help based on the course content.\nHelp with traditional assessment categories as well as coding."
  },
  {
    "objectID": "projects/Gen_AI_Tutor.html#project-overview",
    "href": "projects/Gen_AI_Tutor.html#project-overview",
    "title": "Custom Gen AI Tutor for Linear Algebra",
    "section": "",
    "text": "In Fall 2025, together with the office of Strategic Initiatives and Planning at NYU IT, we initiated a pilot program to incorporate Gen. AI Tutor trained on the textbook and other course-related documents, for one section of Linear Algebra. The tutor is designed to give only hints and suggestions and never the full answers right away. It guides the students towards solutions, which effectively improves their understanding of the material. Analysis of the data collected from that semester is underway and will be used to refine the Tutor for the upcoming Spring 2026 semester.\n\n\n\nCreate a Personalized Course Tutor.\nGuided and focused study help based on the course content.\nHelp with traditional assessment categories as well as coding."
  },
  {
    "objectID": "projects/CURE.html",
    "href": "projects/CURE.html",
    "title": "Course-Based Undergraduate Research Framework (CURE)",
    "section": "",
    "text": "Starting Fall 2025, as a part of the working group headed by Prof. Rohini Qamra, I learned about the CURE framework and discussed ways to implement it in math courses. It is a challenging proposition to incorporate full CURE framework for a typical math course with 120 students. In Fall, I did a soft version in Linear Algebra with Applications via coding projects at the end of the semester. In Spring 20206, I plan to implement a similar project-based learning in both of the Core classes I am teaching: From data To Discovery and Great Ideas In Mathematics.\n\n\n\nIncorporate Reserach component in traditional courses.\nEncourage curiosity, exploration, and understanding.\nGive students a taste of what research is like.\nBuild skills like critical thinking, teamwork, and research mindset."
  },
  {
    "objectID": "projects/CURE.html#project-overview",
    "href": "projects/CURE.html#project-overview",
    "title": "Course-Based Undergraduate Research Framework (CURE)",
    "section": "",
    "text": "Starting Fall 2025, as a part of the working group headed by Prof. Rohini Qamra, I learned about the CURE framework and discussed ways to implement it in math courses. It is a challenging proposition to incorporate full CURE framework for a typical math course with 120 students. In Fall, I did a soft version in Linear Algebra with Applications via coding projects at the end of the semester. In Spring 20206, I plan to implement a similar project-based learning in both of the Core classes I am teaching: From data To Discovery and Great Ideas In Mathematics.\n\n\n\nIncorporate Reserach component in traditional courses.\nEncourage curiosity, exploration, and understanding.\nGive students a taste of what research is like.\nBuild skills like critical thinking, teamwork, and research mindset."
  },
  {
    "objectID": "talks/nypl-physics-2018.html",
    "href": "talks/nypl-physics-2018.html",
    "title": "The Physics of Everyday Things",
    "section": "",
    "text": "When we think of the world of physics, we usually think either very big, like stars and galaxies, or very small like atoms and molecules. But our everyday experience typically is of things that are somewhere in-between in scale. Think of most objects that we deal with on a daily basis - glassware and salt and pepper, soaps, shampoos, toothpaste, ketchup, etc.. The physics of these kinds of objects and materials is in fact an active area of research and is classified as “soft matter physics”.\nIn this series, we will explore the physics behind a few of these materials.\nFirst, we will see that a simple and ubiquitous material like sand is in fact very complicated and can exhibit puzzling behavior. Sand or anything that is “grainy” (think of nuts and grains in those mini-silos in your supermarket) can show behavior that is solid-like (will stay as is if not disturbed), liquid-like (can be poured) or gas-like (think of sand storms). Physics of grainy materials in each of these states is unique and we will see why they behave the way they do and also why it is important to study such systems (from efficient storage to making the Mars Rover move on Martian sand efficiently).\nNext we will explore the world of strange “liquids”. We know what we mean when we think of liquids - things that are like water, can be poured, if poured, it flows away etc. But how about mayonnaise, ketchup, or even toothpaste? Should they be considered liquids? Or are they somewhere between solids and liquids. These kinds of unusual fluids, where a simple liquid is mixed-in with some other “stuff” are known as “complex fluids”. It is this mix of different things in them that gives rise to unusual properties they show. [cite_start]We will see what those properties are and how to understand their behavior.\nFinally, we will explore the emerging field of creating new materials and artificial machines inspired by nature. For example, what allows a gecko to crawl vertically up? How do we make super light and yet super strong material like spider silk. Can we create tiny nano-bots that can be programmed and injected in our body to perform repairs to injuries and heal diseases? [cite_start]We will explore cutting edge research on many such new inventions."
  },
  {
    "objectID": "talks/nypl-physics-2018.html#abstract",
    "href": "talks/nypl-physics-2018.html#abstract",
    "title": "The Physics of Everyday Things",
    "section": "",
    "text": "When we think of the world of physics, we usually think either very big, like stars and galaxies, or very small like atoms and molecules. But our everyday experience typically is of things that are somewhere in-between in scale. Think of most objects that we deal with on a daily basis - glassware and salt and pepper, soaps, shampoos, toothpaste, ketchup, etc.. The physics of these kinds of objects and materials is in fact an active area of research and is classified as “soft matter physics”.\nIn this series, we will explore the physics behind a few of these materials.\nFirst, we will see that a simple and ubiquitous material like sand is in fact very complicated and can exhibit puzzling behavior. Sand or anything that is “grainy” (think of nuts and grains in those mini-silos in your supermarket) can show behavior that is solid-like (will stay as is if not disturbed), liquid-like (can be poured) or gas-like (think of sand storms). Physics of grainy materials in each of these states is unique and we will see why they behave the way they do and also why it is important to study such systems (from efficient storage to making the Mars Rover move on Martian sand efficiently).\nNext we will explore the world of strange “liquids”. We know what we mean when we think of liquids - things that are like water, can be poured, if poured, it flows away etc. But how about mayonnaise, ketchup, or even toothpaste? Should they be considered liquids? Or are they somewhere between solids and liquids. These kinds of unusual fluids, where a simple liquid is mixed-in with some other “stuff” are known as “complex fluids”. It is this mix of different things in them that gives rise to unusual properties they show. [cite_start]We will see what those properties are and how to understand their behavior.\nFinally, we will explore the emerging field of creating new materials and artificial machines inspired by nature. For example, what allows a gecko to crawl vertically up? How do we make super light and yet super strong material like spider silk. Can we create tiny nano-bots that can be programmed and injected in our body to perform repairs to injuries and heal diseases? [cite_start]We will explore cutting edge research on many such new inventions."
  },
  {
    "objectID": "talks/nypl-physics-2018.html#event-details",
    "href": "talks/nypl-physics-2018.html#event-details",
    "title": "The Physics of Everyday Things",
    "section": "Event Details",
    "text": "Event Details\nLocation: Jefferson Branch, Manhattan Audience: All ages."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Clinical Associate Professor of Mathematics",
    "section": "",
    "text": "WarningUnder Construction\n\n\n\nPlease note that I am currently populating the site. Some sections may be blank. Please be patient."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Clinical Associate Professor of Mathematics",
    "section": "Bio",
    "text": "Bio\nI am a Clinical Associate Professor of Mathematics at the Courant Institute, New York University. “Clinical” is our designation for Teaching Track faculty. I have been teaching here since 2011 and have taught close to 8000 students. My primary job is teaching and coordinating undergraduate courses, and training new instructors and teaching assistants. I teach three undergraduate courses per semester (usually fairly large classes). I also do a lot of outreach for high school students, locally, nationally, and internationally. My longtime passion project is to encourage undergrads and high school students in research. I also try to include a short research component in my courses through projects. I am passionate about open-source education and modernizing the mathematics curriculum.\nMy research has spanned a number of different fields over the years. My background is in Physics, where I have worked on grvitational waves, pattern formation in nonlinear systems, Bose-Einstein condensation, granular physics, complex fluids, and locomotion of micro-organisms. Recently, I have been interested in the intersection of physics, math, and deep learning. I like to take up small projects, suitable for undergraduates, to investigate deep learning methods and their foundations.\nOutside of teaching and research, I am on the scientific advisory board of Synaptrix Labs. Synaprix Labs is a startup created by a former student Aryan Govil. Their vision is to utilize Brain-Computer Interface (BCI) advances to create a wheelchair for ALS patients that can control and direct motion through real-time EEG analysis."
  },
  {
    "objectID": "index.html#education-and-certifications",
    "href": "index.html#education-and-certifications",
    "title": "Clinical Associate Professor of Mathematics",
    "section": "Education and Certifications",
    "text": "Education and Certifications\nPh.D. in Physics** | Duke University | 2006\nM.SC. in Physics** | University of Pune | 1995\nB.Sc. in Physics** | University of Mumbai| 1993\nCertification: Applied Data Science (Leveraging AI for Decision Making)| MIT Professional Education | 2023\nCertification: Data Science and AI Educators’ Program| Alan Turing Institute, UK | 2023"
  },
  {
    "objectID": "index.html#professional-experience",
    "href": "index.html#professional-experience",
    "title": "Clinical Associate Professor of Mathematics",
    "section": "Professional Experience",
    "text": "Professional Experience\nNew York University, Department of Mathematics\nClinical Associate Professor | Sept 2024 – Present\nClinical Assistant Professor | Sept 2012 – Sept 2024\nVisiting Assistant Professor | Sept 2011 – Sept 2012\nCourant Instructor / Post-doctoral Scientist | Sept 2009 – Sept 2011\nMassachusetts Institute of Technology (MIT)\nPost-doctoral Research Fellow, Mechanical Engineering | 2006 – 2009"
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Clinical Associate Professor of Mathematics",
    "section": "News",
    "text": "News\n\nJan 2026: I will be teaching Linear Algebra, and two Core classes: From Data To Discovery and Great Ideas In Mathematics."
  },
  {
    "objectID": "courses/FDTD.html",
    "href": "courses/FDTD.html",
    "title": "From Data To Discovery",
    "section": "",
    "text": "Syllabus (PDF)\nLecture Notes"
  },
  {
    "objectID": "courses/FDTD.html#course-materials",
    "href": "courses/FDTD.html#course-materials",
    "title": "From Data To Discovery",
    "section": "",
    "text": "Syllabus (PDF)\nLecture Notes"
  },
  {
    "objectID": "outreach/pioneer.html",
    "href": "outreach/pioneer.html",
    "title": "Pioneer Academy",
    "section": "",
    "text": "Tairan Ma: “Multimodal Machine Learning for Physiological Privacy Protection in Facial Videos”\nYichen Leng: “Data Analytics for Speed-Cubing Rankings”\nMohammed Ali: “Machine Learning for Memristor Networks”\nXiaodan (Amy) He: “Convolutional Neural Networks for Facial Expression Recognition: Analysis of Current CNNs and Integration of AI Enhancement”\nTerry Duan: “Retrieval-Augmented Generation for Intelligent Learning Tools”"
  },
  {
    "objectID": "outreach/pioneer.html#participants",
    "href": "outreach/pioneer.html#participants",
    "title": "Pioneer Academy",
    "section": "",
    "text": "Tairan Ma: “Multimodal Machine Learning for Physiological Privacy Protection in Facial Videos”\nYichen Leng: “Data Analytics for Speed-Cubing Rankings”\nMohammed Ali: “Machine Learning for Memristor Networks”\nXiaodan (Amy) He: “Convolutional Neural Networks for Facial Expression Recognition: Analysis of Current CNNs and Integration of AI Enhancement”\nTerry Duan: “Retrieval-Augmented Generation for Intelligent Learning Tools”"
  },
  {
    "objectID": "outreach/pioneer.html#syllabus",
    "href": "outreach/pioneer.html#syllabus",
    "title": "Pioneer Academy",
    "section": "Syllabus",
    "text": "Syllabus\nProgram Title: Machine Learning applied to Scientific Problems Pioneer Syllabus Term: Summer 2025\nProgram Topic Description:\nThis course introduces the students to Machine Learning methods to solve scientific problems. Traditionally, every scientific field has evolved its own methods to tackle problems – whether it is modeling and solving differential equations, using statistics, doing regression analysis and figure out prominent trends etc. Examples of these methods are found everywhere from physics, chemistry, biology, mathematics, astronomy, and even newer fields like climate analysis, renewable energy systems. With the advent of data driven methods and Machine Learning, many of these same methods are merged and most scientific problems are now attacked by employing all these methods in combination.\nWe will be using a combination of lecture notes uploaded on the LMS, pen-paper calculations, and running Python codes via Jupyter Notebooks.\nWe will then go through several standard machine learning methods:\n\nBroad Types: Supervised and Unsupervised\nLinear and Logistic Regression\nClassification methods: Decision Trees, Random Forests, K-Nearest Neighbors, Support Vector Machines.\nArtificial Neural Networks: Feed Forward network, Convolutional Neural Network, Recurrent Neural Network.\nPhysics Informed Neural Networks and Echo State Networks.\nTime Series Methods: LSTM, GRU.\nLarge Language Models and Transformers.\n\nThese methods will allow us to tackle pretty much any problem in scientific fields, and they also form the basis of solving very serious problems like cancer detection to detecting livable exoplanets. In individual sessions, students will consult with the instructor and create a project that best aligns with their interests. It could be in any scientific field. The project should involve one of the machine learning methods applied to study the problem. The student will write up a report/paper about their findings. Preferred writing software is LaTeX for scientific writing, but Word is acceptable. If you want to use LaTeX, I can provide a tutorial on how to use it."
  }
]