{"title":"Physics in Machine Learning, Deep Learning, and AI.","markdown":{"yaml":{"title":"Physics in Machine Learning, Deep Learning, and AI.","date":"2025-08-08","categories":["physics","ai"],"canonical-url":"https://trushphysicsai.substack.com/p/physics-in-machine-learning-deep"},"containsRefs":false,"markdown":"\n\nLet’s see if the following scenario sounds familiar or relatable:\n\nYou are a current student who realized that you need to learn about Machine Learning (ML), Deep Learning (DL), and AI in general. Maybe you are a mid-career professional who is looking to pivot, realizing the same or maybe you have to learn something about these things because you got “the memo” that you will be working with newly hired AI expert!\n\nChances are that irrespective of the exact situation, we all end up following a similar plan of action: look at free resources on the internet, Youtube videos, Lecture notes of famous courses, maybe a bootcamp or two, maybe a paid course if you can afford to. After all that, you feel reasonably secure that you know when to apply which method, why different subject areas like computer vision or healthcare require specific methods suited to those areas. You may even attempt to read original research papers on the methods you intend to use. That’s when you start realizing that the entire structure is heavily steeped in Math!\n\nYou had a feeling about it but you pushed it back to be dealt with later. Now, you start to notice how many people were talking about getting a foundation in math to be able to use ML/DL/AI more effectively and interpret the results properly. So you dust off the old books on calculus, linear algebra, and probability and statistics and repeat the whole cycle mentioned above, but this time for math. How much math you need to do depends on your own individual case. Unless you are a researcher or an engineer tweaking these systems at the fundamental level, you only need an intuitive grasp of the concepts and how they become the foundation of ever complicated architectures.\n\nFinally, you feel you are set and have a reasonable foundation to build upon and go deeper. As soon as you do that, you encounter technical terms, jargon and concepts that were never explained in any material you studied or any math you learned! You come across terms like ensemble, temperature, entropy, partitions, free energy, attractors, stability, equilibrium, non-equilibrium, diffusion, manifolds, groups, symmetries, eigenspaces, dynamics, variational, renormalization, etc. You may even hear someone mention Ising Model, quantization, or pretty much anything with quantum as a prefix.\n\nNow you despair that also need to know Physics!\n\nIt does feel like this whole thing is a bottomless pit but there is a good reason for the need to know a bit of physics. All of the terms I mentioned above and many more are at the foundation of this whole field, starting with neural networks. Most of them are concepts borrowed from Physics and applied to DS/ML/DL/AI. It is a fact that starting with “neurons” in neural networks, a whole bunch of methods, architectures, and protocols are new applications of statistical physics. Occasionally, well-established methods from astronomy and particle physics have also been modified and adapted.\n\nIt is not so surprising that the Nobel Prize for advances in AI was awarded in the Physics category. Now, it may even be clearer why notable figures like Elon Musk and Jensen Huang have been advocating studying physics.\n\nI have been observing and thinking about this need to explain the physics that operates behind-the-scenes for a while now. In subsequent posts, I aim to comprehensively explain each and every physics concept that is used in AI. I am not attempting to convert anyone to study physics endlessly or become an expert at it. My goal for this blog is to give you intuitive understanding and sufficient insights so that if someone tries to “blind you with physics”, you can hold your own!\n\nLet me briefly explain why I think I can do this. Firstly, I love teaching, am passionate about it. Majority of my teaching has been in Math and Physics. I am trained as a soft matter physicist, but at various points in my life, I have worked in astrophysics, nonlinear dynamics and chaos, pattern formation, Bose-Einstein condensation, physics of granular systems, complex fluids, and locomotion of microorganisms. That basically covers the entire length scale, from cosmic scale to quantum scale and the in-between scale.\n\nI transitioned to a teaching career thirteen years ago. I am a teaching-track math professor at NYU, Courant Institute. I have taught math to an incredibly diverse group of undergraduates. I have taught abstract math concepts to every possible major on campus. I enjoy finding ways to explain and engage students who are usually apprehensive and sometimes terrified of learning math. I plan on bringing the same passion here and to quote Einstein, “Make everything as simple as possible but not simpler.”\n\nI genuinely hope you enjoy my offering and maybe even get some practical benefit out of it. I know I am going to thoroughly enjoy this new creative outlet.\n\n\n\n","srcMarkdownNoYaml":"\n\nLet’s see if the following scenario sounds familiar or relatable:\n\nYou are a current student who realized that you need to learn about Machine Learning (ML), Deep Learning (DL), and AI in general. Maybe you are a mid-career professional who is looking to pivot, realizing the same or maybe you have to learn something about these things because you got “the memo” that you will be working with newly hired AI expert!\n\nChances are that irrespective of the exact situation, we all end up following a similar plan of action: look at free resources on the internet, Youtube videos, Lecture notes of famous courses, maybe a bootcamp or two, maybe a paid course if you can afford to. After all that, you feel reasonably secure that you know when to apply which method, why different subject areas like computer vision or healthcare require specific methods suited to those areas. You may even attempt to read original research papers on the methods you intend to use. That’s when you start realizing that the entire structure is heavily steeped in Math!\n\nYou had a feeling about it but you pushed it back to be dealt with later. Now, you start to notice how many people were talking about getting a foundation in math to be able to use ML/DL/AI more effectively and interpret the results properly. So you dust off the old books on calculus, linear algebra, and probability and statistics and repeat the whole cycle mentioned above, but this time for math. How much math you need to do depends on your own individual case. Unless you are a researcher or an engineer tweaking these systems at the fundamental level, you only need an intuitive grasp of the concepts and how they become the foundation of ever complicated architectures.\n\nFinally, you feel you are set and have a reasonable foundation to build upon and go deeper. As soon as you do that, you encounter technical terms, jargon and concepts that were never explained in any material you studied or any math you learned! You come across terms like ensemble, temperature, entropy, partitions, free energy, attractors, stability, equilibrium, non-equilibrium, diffusion, manifolds, groups, symmetries, eigenspaces, dynamics, variational, renormalization, etc. You may even hear someone mention Ising Model, quantization, or pretty much anything with quantum as a prefix.\n\nNow you despair that also need to know Physics!\n\nIt does feel like this whole thing is a bottomless pit but there is a good reason for the need to know a bit of physics. All of the terms I mentioned above and many more are at the foundation of this whole field, starting with neural networks. Most of them are concepts borrowed from Physics and applied to DS/ML/DL/AI. It is a fact that starting with “neurons” in neural networks, a whole bunch of methods, architectures, and protocols are new applications of statistical physics. Occasionally, well-established methods from astronomy and particle physics have also been modified and adapted.\n\nIt is not so surprising that the Nobel Prize for advances in AI was awarded in the Physics category. Now, it may even be clearer why notable figures like Elon Musk and Jensen Huang have been advocating studying physics.\n\nI have been observing and thinking about this need to explain the physics that operates behind-the-scenes for a while now. In subsequent posts, I aim to comprehensively explain each and every physics concept that is used in AI. I am not attempting to convert anyone to study physics endlessly or become an expert at it. My goal for this blog is to give you intuitive understanding and sufficient insights so that if someone tries to “blind you with physics”, you can hold your own!\n\nLet me briefly explain why I think I can do this. Firstly, I love teaching, am passionate about it. Majority of my teaching has been in Math and Physics. I am trained as a soft matter physicist, but at various points in my life, I have worked in astrophysics, nonlinear dynamics and chaos, pattern formation, Bose-Einstein condensation, physics of granular systems, complex fluids, and locomotion of microorganisms. That basically covers the entire length scale, from cosmic scale to quantum scale and the in-between scale.\n\nI transitioned to a teaching career thirteen years ago. I am a teaching-track math professor at NYU, Courant Institute. I have taught math to an incredibly diverse group of undergraduates. I have taught abstract math concepts to every possible major on campus. I enjoy finding ways to explain and engage students who are usually apprehensive and sometimes terrified of learning math. I plan on bringing the same passion here and to quote Einstein, “Make everything as simple as possible but not simpler.”\n\nI genuinely hope you enjoy my offering and maybe even get some practical benefit out of it. I know I am going to thoroughly enjoy this new creative outlet.\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"canonical-url":"https://trushphysicsai.substack.com/p/physics-in-machine-learning-deep"},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"html-math-method":"mathjax","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":"litera","title":"Physics in Machine Learning, Deep Learning, and AI.","date":"2025-08-08","categories":["physics","ai"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}